for(i in colnames(mat)[5:85]) {
for(j in mat$Identification.Code) {
mat.val <- mat[which(mat$Identification.Code==j), which(colnames(mat)==i)]
if(mat.val > 0) {
next
} else {
id <- substring(j, 15, 27)
cur.val <- current[which(current$DPID==id), which(colnames(current)==i)]
if(cur.val==0) {
current[which(current$DPID==id), which(colnames(current)==i)] <- NA
} else {
print(paste("Data available for ", id, " at ", i, " but not in matrix", sep=""))
}
}
}
}
current <- data.frame(matrix(0, nrow=180, ncol=83))
colnames(current)[1:2] <- c("DPID","DPName")
colnames(current)[3:83] <- sites[[1]]$siteCode
current$DPID <- ls[[1]]$productCode
current$DPName <- ls[[1]]$productName
for(i in 1:nrow(current)) {
dp.sites <- ls[[1]]$siteCodes[[i]]$siteCode
current[i,which(colnames(current) %in% dp.sites)] <- 1
}
mat <- read.delim("~/sandbox/Master_product_siteMatrix.csv", sep=",", skip=5)
mat <- mat[1:180,1:85]
precip <- apply(t(mat[which(mat$Identification.Code=="NEON.DOM.SITE.DP1.00006.001"),5:85]),
2, as.numeric)
precip <- apply(precip, 1, sum)
mat <- mat[-which(mat$Identification.Code=="NEON.DOM.SITE.DP1.00006.001"),]
mat <- rbind(mat, c("Precipitation", "NEON.DOM.SITE.DP1.00006.001", "TIS", "", precip))
for(i in colnames(mat)[5:85]) {
for(j in mat$Identification.Code) {
mat.val <- mat[which(mat$Identification.Code==j), which(colnames(mat)==i)]
if(mat.val > 0) {
next
} else {
id <- substring(j, 15, 27)
cur.val <- current[which(current$DPID==id), which(colnames(current)==i)]
if(cur.val==0) {
current[which(current$DPID==id), which(colnames(current)==i)] <- NA
} else {
print(paste("Data available for ", id, " at ", i, " but not in matrix", sep=""))
}
}
}
}
samp <- read.delim("~/sandbox/NEON-OS-DataAvailabilityBySiteByYear-v20170918.csv", sep=",")
samp$DPID <- paste(substring(samp$DPID, 2, 10), ".001", sep="")
current.os <- current[which((substring(current$DPID, 5, 5) %in% c(1,2) |
current$DPID %in% c("DP1.00101.001","DP1.00013.001","DP1.00038.001",
"DP4.00131.001","DP4.00132.001","DP4.00133.001",
"DP1.00096.001","DP1.00097.001"))
& !current$DPID %in% c("DP1.20002.001","DP1.20004.001","DP1.20015.001",
"DP1.20016.001","DP1.20032.001","DP1.20033.001",
"DP1.20042.001","DP1.20046.001","DP1.20053.001",
"DP1.20059.001","DP1.20100.001","DP1.20217.001",
"DP1.20261.001","DP1.20264.001","DP1.20271.001",
"DP1.20288.001")),]
for(i in current.os$DPID) {
if(length(which(samp$DPID==i))==0) {
print(paste("No sampling info for", i, current.os$DPName[which(current.os$DPID==i)], sep=" "))
} else {
for(j in colnames(current.os)[3:83]) {
print(paste(i, j))
cur.val <- current.os[which(current.os$DPID==i), which(colnames(current.os)==j)]
if(is.na(cur.val) | cur.val==1) {
next
} else {
samp.dp <- samp[which(samp$DPID==i & samp$Site==j),]
samp.sub <- samp.dp[,grep("X", colnames(samp.dp), fixed=T)]
if(all(is.na(samp.sub) | samp.sub=="")) {
next
} else {
if(nrow(samp.sub)>1) {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y", arr.ind=T)[,2], na.rm=T)]
} else {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y"), na.rm=T)]
}
if(as.numeric(substring(yr, 2, 5)) <= 2017) {
next
} else {
current.os[which(current.os$DPID==i), which(colnames(current.os)==j)] <- substring(yr, 2, 5)
}
}
}
}
}
}
current.os <- current[which((substring(current$DPID, 5, 5) %in% c(1,2) |
current$DPID %in% c("DP1.00101.001","DP1.00013.001","DP1.00038.001",
"DP4.00131.001","DP4.00132.001","DP4.00133.001",
"DP1.00096.001","DP1.00097.001"))
& !current$DPID %in% c("DP1.20002.001","DP1.20004.001","DP1.20015.001",
"DP1.20016.001","DP1.20032.001","DP1.20033.001",
"DP1.20042.001","DP1.20046.001","DP1.20053.001",
"DP1.20059.001","DP1.20100.001","DP1.20217.001",
"DP1.20261.001","DP1.20264.001","DP1.20271.001",
"DP1.20288.001")),]
for(i in current.os$DPID) {
if(length(which(samp$DPID==i))==0) {
print(paste("No sampling info for", i, current.os$DPName[which(current.os$DPID==i)], sep=" "))
} else {
for(j in colnames(current.os)[3:83]) {
#print(paste(i, j))
cur.val <- current.os[which(current.os$DPID==i), which(colnames(current.os)==j)]
if(is.na(cur.val) | cur.val==1) {
next
} else {
samp.dp <- samp[which(samp$DPID==i & samp$Site==j),]
samp.sub <- samp.dp[,grep("X", colnames(samp.dp), fixed=T)]
if(all(is.na(samp.sub) | samp.sub=="")) {
next
} else {
if(nrow(samp.sub)>1) {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y", arr.ind=T)[,2], na.rm=T)]
} else {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y"), na.rm=T)]
}
if(as.numeric(substring(yr, 2, 5)) <= 2017) {
next
} else {
current.os[which(current.os$DPID==i), which(colnames(current.os)==j)] <- substring(yr, 2, 5)
}
}
}
}
}
}
samp <- read.delim("~/sandbox/NEON-OS-DataAvailabilityBySiteByYear-v20170918.csv", sep=",")
samp$DPID <- paste(substring(samp$DPID, 2, 10), ".001", sep="")
current.os <- current[which((substring(current$DPID, 5, 5) %in% c(1,2) |
current$DPID %in% c("DP1.00101.001","DP1.00013.001","DP1.00038.001",
"DP4.00131.001","DP4.00132.001","DP4.00133.001",
"DP1.00096.001","DP1.00097.001"))
& !current$DPID %in% c("DP1.20002.001","DP1.20004.001","DP1.20015.001",
"DP1.20016.001","DP1.20032.001","DP1.20033.001",
"DP1.20042.001","DP1.20046.001","DP1.20053.001",
"DP1.20059.001","DP1.20100.001","DP1.20217.001",
"DP1.20261.001","DP1.20264.001","DP1.20271.001",
"DP1.20288.001")),]
for(i in current.os$DPID) {
if(length(which(samp$DPID==i))==0) {
print(paste("No sampling info for", i, current.os$DPName[which(current.os$DPID==i)], sep=" "))
} else {
for(j in colnames(current.os)[3:83]) {
#print(paste(i, j))
cur.val <- current.os[which(current.os$DPID==i), which(colnames(current.os)==j)]
if(is.na(cur.val) | cur.val==1) {
next
} else {
samp.dp <- samp[which(samp$DPID==i & samp$Site==j),]
samp.sub <- samp.dp[,grep("X", colnames(samp.dp), fixed=T)]
if(all(is.na(samp.sub) | samp.sub=="")) {
next
} else {
if(nrow(samp.sub)>1) {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y", arr.ind=T)[,2], na.rm=T)]
} else {
yr <- colnames(samp.sub)[min(which(samp.sub=="Y"), na.rm=T)]
}
if(as.numeric(substring(yr, 2, 5)) <= 2017) {
next
} else {
current.os[which(current.os$DPID==i), which(colnames(current.os)==j)] <- substring(yr, 2, 5)
}
}
}
}
}
}
write.table(current.os, "~/sandbox/os_status.csv", row.names=F, sep=",")
shiny::runApp('os_qaqc/prototype_message')
runApp('os_qaqc/prototype_message')
View(current)
View(current.os)
View(current.os)
i <- "DP1.10035.001"
j <- "ABBY"
length(which(samp$DPID==i))==0
cur.val <- current.os[which(current.os$DPID==i), which(colnames(current.os)==j)]
cur.val
is.na(cur.val) | cur.val==1
samp[which(samp$DPID==i & samp$Site==j),]
samp.dp <- samp[which(samp$DPID==i & samp$Site==j),]
samp.sub <- samp.dp[,grep("X", colnames(samp.dp), fixed=T)]
samp.sub
all(is.na(samp.sub) | samp.sub=="")
library(httr)
library(httr)
stack <- "prod"
status <- "ERROR"
type <- "FULCRUM"
mes.call <- paste("http://", stack, "-os-ds-1.ci.neoninternal.org:8080/osDataService/messages?",
"message-status-state-code=", status,
# ifelse(is.na(startDate), "", "&jms-message-date-begin="),
# ifelse(is.na(startDate), "", startDate),
# ifelse(is.na(endDate), "", "&jms-message-date-cutoff="),
# ifelse(is.na(endDate), "", endDate),
"&message-type-code=", type,
sep="")
req <- httr::GET(mes.call,
httr::accept("application/vnd.neoninc.os.message-list-v1.0+xml"))
req.x <- XML::xmlParse(httr::content(req, as="text"))
req.l <- XML::xpathApply(req.x, "//message")
req.l[[1]]
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/messages/9d74edf8-1c80-4f8d-b3d3-bcbeb8619f99/message-files?fileUri=920bf53f-007d-4e9c-84da-312bc2c8f4b2/a9c2894d-7c30-4319-a1fa-def6c8efa1d3")
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/messages/9d74edf8-1c80-4f8d-b3d3-bcbeb8619f99/message-files?fileUri=920bf53f-007d-4e9c-84da-312bc2c8f4b2/a9c2894d-7c30-4319-a1fa-def6c8efa1d3",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files/920bf53f-007d-4e9c-84da-312bc2c8f4b2/a9c2894d-7c30-4319-a1fa-def6c8efa1d3",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/messages/9d74edf8-1c80-4f8d-b3d3-bcbeb8619f99/message-files?920bf53f-007d-4e9c-84da-312bc2c8f4b2/99e30026-f2e8-4715-b011-5774770dbaa8",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files/920bf53f-007d-4e9c-84da-312bc2c8f4b2/99e30026-f2e8-4715-b011-5774770dbaa8",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files?fileUri=920bf53f-007d-4e9c-84da-312bc2c8f4b2/99e30026-f2e8-4715-b011-5774770dbaa8",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files/920bf53f-007d-4e9c-84da-312bc2c8f4b2/99e30026-f2e8-4715-b011-5774770dbaa8")
test
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files/99e30026-f2e8-4715-b011-5774770dbaa8",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
req.l[[38]]
library(httr)
stack <- "prod"
status <- "ERROR"
type <- "FULCRUM"
mes.call <- paste("http://", stack, "-os-ds-1.ci.neoninternal.org:8080/osDataService/messages?",
"message-status-state-code=", status,
# ifelse(is.na(startDate), "", "&jms-message-date-begin="),
# ifelse(is.na(startDate), "", startDate),
# ifelse(is.na(endDate), "", "&jms-message-date-cutoff="),
# ifelse(is.na(endDate), "", endDate),
"&message-type-code=", type,
sep="")
req <- httr::GET(mes.call,
httr::accept("application/vnd.neoninc.os.message-list-v1.0+xml"))
req.x <- XML::xmlParse(httr::content(req, as="text"))
req.l <- XML::xpathApply(req.x, "//message")
req.l[[38]]
test <- httr::GET("http://prod-os-ds-1.ci.neoninternal.org:8080/osDataService/message-files/b959f147-b3b5-4dd9-ae88-8189780c1fac/798ca976-84fd-4a89-bc56-1c16ea7438fc",
httr::accept("application/vnd.neoninc.os.file-entry-v1.0+xml"))
test
req.l[[38]]$messageFile
req.l[[38]][["//messageFile"]]
req.l[[38]]["//messageFile"]
req.l[[38]]
XML::xpathApply(req.l[[38]], "//messageFile")
sls <- req.l[[38]]
class(sls)
XML::xpathApply(sls, "//messageFile")
sls
XML::xpathApply(req.x, "//message[38]/messageFile")
length(req.x)
files <- XML::xpathApply(req.x, "//message[38]/messageFile")
files[[1]]
files[[1]]$fileUri
file.l <- XML::xmlToList(files[[1]])
file.l
file.l$fileUri
j.call <- paste("http://prod-os-parser-1.ci.neoninternal.org:8080/osParser/download-fulcrum-message?file-uri=",
file.l$fileUri, sep="")
req.j <- httr::GET(j.call)
req.j
ful.j <- jsonlite::fromJSON(content(req.j, as="text"))
ful.j
length(files)
j.list <- list()
for(i in 1:length(req.l)) {
# get file section from message
files <- XML::xpathApply(req.x, paste("//message[", i, "]/messageFile", sep=""))
for(j in 1:length(files)) {
# get url(s) out of file section and query
file.l <- XML::xmlToList(files[[j]])
j.call <- paste("http://prod-os-parser-1.ci.neoninternal.org:8080/osParser/download-fulcrum-message?file-uri=",
file.l$fileUri, sep="")
req.j <- httr::GET(j.call)
# flatten - this may or may not be the way we want to handle the json files
ful.j <- jsonlite::fromJSON(content(req.j, as="text"))
j.list <- list(j.list, ful.j)
}
}
?list
length(XML::xpathApply(req.x, "//messageFile"))
files <- XML::xpathApply(req.x, "//messageFile")
j.list <- list(length(files))
for(i in 1:length(files)) {
# get url and query
file.l <- XML::xmlToList(files[[i]])
if(file.l$fileUri=="NO_DATA") {
next
} else {
j.call <- paste("http://prod-os-parser-1.ci.neoninternal.org:8080/osParser/download-fulcrum-message?file-uri=",
file.l$fileUri, sep="")
req.j <- httr::GET(j.call)
# flatten - this may or may not be the way we want to handle the json files
ful.j <- jsonlite::fromJSON(httr::content(req.j, as="text", encoding="UTF-8"))
j.list[[i]] <- ful.j
}
}
j.list[[1]]
View(j.list[[1]])
heap <- read.delim("/Users/clunch/Desktop/Plant_phenology_observationsJavaHeap.txt", sep="\t")
View(heap)
View(heap)
?rnorm
?base
nchar("The basic syntax for running R code via `rpy2` is `package.function(inputs)`,")
library(httr)
library(jsonlite)
library(ggplot2)
library(tidyr)
options(stringsAsFactors=F)
# set file path
if(file.exists("/Users/clunch")) {
filepath <- "/Users/clunch/user-analytics/github_traffic/"
}
repo.counts <- read.delim(paste(filepath, "repo_counts.csv", sep=""), sep=",", header=T)
repo.uniques <- read.delim(paste(filepath, "repo_uniques.csv", sep=""), sep=",", header=T)
colnames(repo.counts) <- gsub(".", "-", colnames(repo.counts), fixed=T)
colnames(repo.uniques) <- gsub(".", "-", colnames(repo.uniques), fixed=T)
# list of repos to get stats for
gitList <- c("NEON-utilities","NEON-geolocation",
"NEON-stream-morphology","NEON-stream-discharge",
"NEON-Nitrogen-Transformations","NEON-dissolved-gas",
"NEON-reaeration")
dates <- as.character(rep(Sys.Date(),15)-0:14)
dates <- paste(dates,"T00:00:00Z", sep="")
counts <- data.frame(timestamp=dates)
uniques <- data.frame(timestamp=dates)
for(i in gitList) {
# # get referring pages and their numbers - not yet implemented
# gitRef <- GET(paste("https://api.github.com/repos/NEONScience/", i,
#                     "/traffic/popular/referrers", sep=""),
#               accept="application/vnd.github.v3+json",
#               authenticate(Sys.getenv("GITHUB_TOKEN"), ""))
# refs <- fromJSON(content(gitRef, as="text"), flatten=T)
# get page views
gitView <- GET(paste("https://api.github.com/repos/NEONScience/", i,
"/traffic/views", sep=""),
accept="application/vnd.github.v3+json",
authenticate(Sys.getenv("GITHUB_TOKEN"), ""))
views <- fromJSON(content(gitView, as="text"), flatten=T)
# if no views, merge won't work, need to add column manually
if(length(views$views)==0) {
counts <- cbind(counts, rep(0, nrow(counts)))
colnames(counts)[ncol(counts)] <- i
uniques <- cbind(uniques, rep(0, nrow(uniques)))
colnames(uniques)[ncol(uniques)] <- i
next
}
# merge with other repos' counts
counts <- merge(counts,views$views[,c("timestamp","count")], all=T)
colnames(counts)[ncol(counts)] <- i
# merge with other repos' uniques
uniques <- merge(uniques,views$views[,c("timestamp","uniques")], all=T)
colnames(uniques)[ncol(uniques)] <- i
}
counts[is.na(counts)] <- 0
uniques[is.na(uniques)] <- 0
# append new page views to the count and unique lists
repo.counts <- rbind(repo.counts, counts)
repo.uniques <- rbind(repo.uniques, uniques)
daterep <- repo.counts$timestamp[which(duplicated(repo.counts$timestamp))]
if(length(daterep)>0) {
for(j in daterep) {
repo.counts <- repo.counts[-min(which(repo.counts$timestamp==j)),]
}
}
daterepu <- repo.uniques$timestamp[which(duplicated(repo.uniques$timestamp))]
if(length(daterepu)>0) {
for(j in daterepu) {
repo.uniques <- repo.uniques[-min(which(repo.uniques$timestamp==j)),]
}
}
write.table(repo.counts, paste(filepath, "repo_counts.csv", sep=""), sep=",",
row.names=F)
write.table(repo.uniques, paste(filepath, "repo_uniques.csv", sep=""), sep=",",
row.names=F)
long.counts <- gather(repo.counts, key=repo, value=views, 2:ncol(repo.counts))
long.counts$timestamp <- strptime(long.counts$timestamp, format="%Y-%m-%dT%H:%M:%SZ")
long.uniques <- gather(repo.uniques, key=repo, value=unique.visitors, 2:ncol(repo.uniques))
long.uniques$timestamp <- strptime(long.uniques$timestamp, format="%Y-%m-%dT%H:%M:%SZ")
ggplot(long.counts) + aes(timestamp, views) + facet_grid(repo~.) + geom_line()
ggsave(paste(filepath, "counts_fig.pdf", sep=""))
ggplot(long.uniques) + aes(timestamp, unique.visitors) + facet_grid(repo~.) + geom_line()
ggsave(paste(filepath, "uniques_fig.pdf", sep=""))
install.packages("data.tree")
install.packages("data.tree")
install.packages("data.tree")
install.packages("data.tree")
install.packages("data.tree")
install.packages("data.tree")
library(data.tree)
samp <- read.delim("/Users/clunch/how-to-make-a-data-product/Ingest workbook OS/sample_tree/sampleTree.csv",
sep=",")
data.tree(samp)
data.tree::as.Node(samp)
class(samp)
data.tree::as.Node.data.frame(samp)
?FromDataFrameTable
data.tree::FromDataFrameNetwork(samp)
samp.tree <- data.tree::FromDataFrameNetwork(samp)
plot(samp.tree)
plot(samp.tree)
install.packages("networkD3")
library(networkD3)
simpleNetwork(samp.tree)
simpleNetwork(samp)
simpleNetwork(samp)
?simpleNetwork
simpleNetwork(samp, fontSize=10)
samp <- read.delim("/Users/clunch/how-to-make-a-data-product/Ingest workbook OS/sample_tree/sampleTree.csv",
sep=",")
simpleNetwork(samp, fontSize=10)
samp.tree <- data.tree::FromDataFrameNetwork(samp) # not as helpful
samp <- read.delim("/Users/clunch/how-to-make-a-data-product/Ingest workbook OS/sample_tree/sampleTree.csv",
sep=",")
simpleNetwork(samp, fontSize=10)
forceNetwork(samp)
install.packages(igraph)
install.packages("igraph")
install.packages("igraph")
library(igraph)
library(data.tree)
library(networkD3)
samp <- read.delim("/Users/clunch/how-to-make-a-data-product/Ingest workbook OS/sample_tree/sampleTree.csv",
sep=",")
plot(as.igraph(samp))
i.samp <- graph_from_data_frame(samp)
i.samp
plot(i.samp)
library(neonUtilities)
zipsByProduct("DP4.00200.001",site="UNDE",package="expanded",savepath="/Users/clunch/Desktop")
install.packages("h5")
install.packages("rhdf5")
install.packages("rhdf5")
install.packages("h5")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
unde <- "/Users/clunch/Desktop/filesToStack00200/NEON.D05.UNDE.DP4.00200.001.2017-09.expanded.20180522T213225Z/NEON.D05.UNDE.DP4.00200.001.nsae.2017-09-01.expanded.h5"
h5ls(unde)
install.packages("shiny")
shiny::runApp('os_qaqc/prototype_message')
install.packages("shinydashboard")
runApp('os_qaqc/prototype_message')
install.packages("DT")
runApp('os_qaqc/prototype_message')
library(devtools)
install.packages("devtools")
library(devtools)
setwd("~/how-to-make-a-data-product/REST_R/restR")
install(".")
install_github("NEONScience/NEON-utilities/neonUtilities")
runApp('~/os_qaqc/prototype_message')
setwd("~/sandbox")
test <- h5read(unde, "/UNDE/dp01/data/co2Stor/000_030_30m")
test
test <- h5read(unde, "/UNDE/dp01/data/co2Stor/000_030_30m/rtioMoleDryCo2")
test
co2 <- h5read(unde, "/UNDE/dp01/data/co2Stor/000_030_30m/rtioMoleDryCo2")
class(co2$timeEnd)
plot(co2$mean~co2$timeEnd)
dt <- strptime(co2$timeEnd, format="%Y-%m-%dT%H:%M:%OS3Z")
dt <- substring(as.character(co2$timeEnd), 1, 19)
dt <- strptime(dt, format="%Y-%m-%dT%H:%M:%S")
plot(co2$mean~dt)
dt
class(dt)
dt <- as.POSIXct(strptime(dt, format="%Y-%m-%dT%H:%M:%S"))
dt <- substring(as.character(co2$timeEnd), 1, 19)
dt <- strptime(dt, format="%Y-%m-%dT%H:%M:%S")
plot(co2$mean)
dt <- substring(as.character(co2$timeEnd), 1, 19)
dt <- as.POSIXct(dt, format="%Y-%m-%dT%H:%M:%S", tz="GMT")
plot(co2$mean~dt)
plot(co2$mean~dt, type="l")
h5readAttributes(unde,"/UNDE/dp01/data/co2Stor/000_030_30m/rtioMoleDryCo2")
read.delim("/Users/clunch/Dropbox/data/FLUXNET/FLX_US-Me2_FLUXNET2015_SUBSET_2002-2014_1-3/FLX_US-Me2_FLUXNET2015_AUXNEE_2002-2014_1-3.csv",
sep=",")
metolius.nee <- read.delim("/Users/clunch/Dropbox/data/FLUXNET/FLX_US-Me2_FLUXNET2015_SUBSET_2002-2014_1-3/FLX_US-Me2_FLUXNET2015_AUXNEE_2002-2014_1-3.csv",
sep=",")
head(metolius.nee)
metolius.daily <- read.delim("/Users/clunch/Dropbox/data/FLUXNET/FLX_US-Me2_FLUXNET2015_SUBSET_2002-2014_1-3/FLX_US-Me2_FLUXNET2015_SUBSET_DD_2002-2014_1-3.csv",
sep=",")
head(metolius.daily)
h5ls(unde)
# NEE data
nee <- h5read(unde, "/UNDE/dp04/data/fluxCo2nsae")
# NEE data
nee <- h5read(unde, "/UNDE/dp04/data/fluxCo2/nsae")
head(nee)
dt <- substring(as.character(nee$timeEnd), 1, 19)
dt <- as.POSIXct(dt, format="%Y-%m-%dT%H:%M:%S", tz="GMT")
plot(nee$flux~dt, type="l")
# join NEE data with quality & uncertainty
nee.u <- h5read(unde, "/UNDE/dp04/ucrt/fluxCo2/nsae")
# join NEE data with quality & uncertainty - some are missing? try out just for storage
stor <- h5read(unde, "/UNDE/dp04/data/fluxCo2/stor")
stor.u <- h5read(unde, "/UNDE/dp04/qfqm/fluxCo2/stor")
head(stor)
head(stor.u)
stor.q <- h5read(unde, "/UNDE/dp04/qfqm/fluxCo2/stor")
head(stor.q)
