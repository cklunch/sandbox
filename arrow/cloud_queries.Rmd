---
title: "Exploring arrow query capabilities"
output: html_document
date: "2025-08-15"
---

```{r setup, include=FALSE}
library(reticulate)
knitr::opts_chunk$set(dev="png")
```

This document explores using arrow functionality to query NEON data without downloading. Accessing data this way can be much faster than downloading everything, with some caveats - see the Speed and Performance section below. The examples used to illustrate the functionality also demonstrate some of the risks and pitfalls, and lead to a set of recommendations at the end of the document.

For background on the software backbone we are using for this, see [https://arrow.apache.org/docs/r/articles/data_wrangling.html](https://arrow.apache.org/docs/r/articles/data_wrangling.html) for R information and [https://duckdb.org/docs/stable/guides/python/sql_on_arrow.html](https://duckdb.org/docs/stable/guides/python/sql_on_arrow.html) for Python.

### Install Packages {.tabset}

#### R

```{r install, eval=FALSE}

install.packages("neonUtilities")
install.packages("dplyr")

```

#### Python

```{python p-install, eval=FALSE}

pip install neonutilities
pip install duckdb

```

### {-}

### Load packages {.tabset}

#### R

```{r R-library, results="hide", message=FALSE}

library(neonUtilities)
library(dplyr)

```

#### Python

```{python p-import}

import neonutilities as nu
import duckdb
import matplotlib.pyplot as plt

```

### {-}

# 1. How to: Capabilities and syntax

### Bind files into a dataset {.tabset}

The arrow software enables us to query many files in a cloud bucket as if they are a single dataset. But first, we have to point it to the set of files we'll be querying, and define those as the dataset of interest. For NEON data, that generally means choosing the site(s), date(s), and table of interest.

Because we're running database-style queries, we can only query one data table at a time. However, see below to learn how to join tables on the fly using these tools.

For our first example, we'll get the mammal trapping data for all time from Treehaven (TREE) in RELEASE-2025.

#### R

```{r dq-mam}

mamds <- datasetQuery(dpID="DP1.10072.001", 
                      site="TREE", package="basic",
                      tabl="mam_pertrapnight",
                      release="RELEASE-2025",
                      token=Sys.getenv("NEON_TOKEN"))

```

#### Python

```{python p-dq-mam}

mamds = nu.dataset_query(dpid="DP1.10072.001", 
                         site="TREE", package="basic",
                         tabl="mam_pertrapnight",
                         release="RELEASE-2025")

```

### {-}

### Database-style query of dataset {.tabset}

We've now defined our dataset without downloading anything. We can use normal `dplyr` syntax in R, and SQL syntax in Python, to query the dataset and download only the data that match the query. Let's say we want to know which species have been captured, and in what numbers: let's get the tag and taxonomic identification for each record, and reduce to unique records to account for recaptures of the same individual.

#### R

```{r mam-ind-tax}

mamTREE <- mamds |> 
  filter(!is.na(taxonID)) |> 
  select(tagID, taxonID, scientificName) |>
  distinct() |>
  collect()

```

#### Python

```{python p-mam-ind-tax}

con = duckdb.connect()
mamTREE = con.execute('''
                      SELECT DISTINCT tagID, taxonID, scientificName 
                      FROM mamds WHERE taxonID != ''
                      ''').df()

```

### {-}

We've easily accessed data on the distribution of taxa, without downloading all the rest of the associated data. Let's make a simple plot of the distribution.

### {.tabset}

#### R

```{r mam-plot-tax}

ct <- table(mamTREE$taxonID)

barplot(ct[order(ct, decreasing=T)], 
        horiz=T, las=1, cex.names=0.5)

```

#### Python

```{python p-mam-plot-tax}

ct = mamTREE.taxonID.value_counts()

fig, ax = plt.subplots()
ax.barh(y=ct.keys(), width=ct)
plt.show()

```

### {-}

### Joining data tables {.tabset}

As noted above, we can only query one data table at a time. However, we can use database-style joining functionality as well, and join tables as we access them. Let's find the mammal species at TREE that have tested positive for tick-borne diseases. This will require querying the rodent pathogen data product (Rodent pathogen status, tick-borne (DP1.10064.002)) as well as the mammal trapping data, and joining the results.

First define the dataset for rodent pathogens at TREE.

#### R

```{r dq-path}

pthds <- datasetQuery(dpID="DP1.10064.002", 
                      site="TREE", package="basic",
                      tabl="rpt2_pathogentesting",
                      release="RELEASE-2025",
                      token=Sys.getenv("NEON_TOKEN"))

```

#### Python

```{python p-dq-path}

pthds = nu.dataset_query(dpid="DP1.10064.002", 
                         site="TREE", package="basic",
                         tabl="rpt2_pathogentesting",
                         release="RELEASE-2025")
                         
```

### {-}

### {.tabset}

Now we join the pathogen data to the mammal trapping dataset and filter to positive pathogen tests. Note that the mammal input at this step is the arrow dataset - the set of files in the cloud - not the data table we downloaded earlier. As with the data queries, we use standard `dplyr` or SQL syntax.

From the joined data, take a look at which taxa have tested positive for any pathogen.

#### R

```{r join-path}

mampath <- mamds |> 
  select(tagID, taxonID, scientificName, bloodSampleID) |> 
  inner_join(pthds, by=c('bloodSampleID' = 'sampleID')) |>
  filter(testResult=='Positive') |>
  distinct() |>
  collect()

table(mampath$scientificName)

```

#### Python

```{python p-join-path}

mampath = con.execute('''
                      SELECT tagID, taxonID, scientificName, bloodSampleID 
                      FROM mamds 
                      INNER JOIN pthds 
                      ON mamds.bloodSampleID = pthds.sampleID 
                      WHERE testResult = 'Positive'
                      ''').df()

mampath.scientificName.value_counts()

```

### {-}

### Access and query NEON sensor data {.tabset}

The examples we've explored so far have been observational (OS) data. These methods are extremely appealing for use with NEON sensor (IS) data, because IS data volumes are large, and can be time-consuming to download and resource-intensive to process. Being able to subset before downloading has the potential to make working with these data much faster and easier.

BUT: there are major limitations due to the way IS data files are structured and named. One of the biggest issues is that IS data don't contain location information - the site, horizontal index, and vertical index - as data fields, but there can be many sensors per site. Because the location information is only in the file name, the query tools can't distinguish data collected by different sensors.

To avoid queries running across sensors, the `datasetQuery()` (R) and `dataset_query()` (Python) functions require that sensor datasets be defined only for a single sensor. `site`, `hor`, and `ver` are all required inputs for sensor data products, and can only be of length 1.

Even with access restricted to one sensor, these tools can be very useful. Let's say we want an estimate of soil moisture at San Joaquin Experimental Range (SJER) in 2023, and let's say we don't need our estimate to be extremely precise, either in space or in time. One sensor at a 30-minute frequency is fine. On the NEON Data Portal, our only option would be to download all Soil water content and salinity (DP1.00094.001) data for 2023. That is, 5 soil plots times 8 soil depths = 40 sensors, each at both 1- and 30-minute averaging intervals. It adds up to 2.26 GB.

Instead, let's get just the 30-minute dataset for the sensor at HOR=002 (soil plot 2) and VER=501 (soil depth 1).

#### R

```{r sw-dq}

swds <- datasetQuery(dpID="DP1.00094.001", 
                     site="SJER", package="basic",
                     hor="002", ver="501",
                     startdate="2023-01", enddate="2023-12",
                     tabl="SWS_30_minute",
                     release="RELEASE-2025")

```

#### Python

```{python p-sw-dq}

swds = nu.dataset_query(dpid="DP1.00094.001", 
                         site="SJER", package="basic",
                         hor="002", ver="501",
                         startdate="2023-01", enddate="2023-12",
                         tabl="SWS_30_minute",
                         release="RELEASE-2025")

```

### {-}

We want all the data from this sensor, so instead of running a query to subset, we return the full data table, and plot a time series of the results.

### {.tabset}

#### R

```{r sw-coll}

swSJER <- swds |>
  collect()

plot(swSJER$VSWCMean~swSJER$endDateTime, type="l")

```

#### Python

Note that the SQL statement here is enclosed in double quotes, rather than 3 single quotes as in the queries above. Since this query is short, it fits easily on one line and is very readable. Using 3 single quotes lets us use multiple lines for the query.

```{python p-sw-coll}

swSJER = con.execute("SELECT * FROM swds").df()

fig, ax = plt.subplots()
ax.plot(swSJER.endDateTime, swSJER.VSWCMean)
plt.show()

```

### {-}

### Averaging on the fly {.tabset}

For sensor data, users are often interested in aggregating data to longer averaging intervals. It's possible to do this on the fly, before downloading, using these query tools, but there are some significant caveats.

Let's say we want the daily mean soil moisture from SJER, instead of the 30-minute data. The first important consideration: we'll omit records with the final quality flag (`VSWCFinalQF`) raised.

#### R

```{r sw-mean}

swmean <- swds |> 
  filter(VSWCFinalQF==0) |>
  select(endDateTime, VSWCMean) |> 
  mutate(dat = date(endDateTime)) |>
  group_by(dat) |>
  summarize(swdaily = mean(VSWCMean, na.rm=T)) |>
  arrange(dat) |>
  collect()

plot(swmean$swdaily~swmean$dat, type="l")

```

#### Python

```{python p-sw-mean}

swmean = con.execute('''
                     SELECT 
                        EXTRACT(DAYOFYEAR FROM endDateTime) AS dat,
                        AVG(VSWCMean) AS swdaily
                     FROM swds 
                     WHERE VSWCFinalQF = 0 
                     GROUP BY 
                        EXTRACT(DAYOFYEAR FROM endDateTime)
                     ORDER BY dat
                     ''').df()

fig, ax = plt.subplots()
ax.plot(swmean.dat, swmean.swdaily)
plt.show()

```

### {-}

The catch is that although we've omitted flagged data records, we don't know which time periods were affected, nor how much data was omitted. The records were left out of the mean silently. Let's make another calculation: the sum of the absolute value of `VSWCFinalQF`. Most sensor data quality flags at NEON can take 3 possible values: 0 = pass, 1 = fail, -1 = could not be evaluated. Summing the absolute value will give us a count of records that did not pass on each day.

Plot the flag count on top of the daily mean data.

### {.tabset}

#### R

```{r sw-mean-flag}

swmeanf <- swds |> 
  select(endDateTime, VSWCFinalQF) |> 
  mutate(dat = date(endDateTime)) |>
  group_by(dat) |>
  summarize(flagdaily = sum(abs(VSWCFinalQF)), na.rm=F) |>
  arrange(dat) |>
  collect()

layout <- layout(matrix(c(1,2), 2, 1, byrow = F)) 
layout <- plot(swmean$swdaily~swmean$dat, type="l")
layout <- plot(swmeanf$flagdaily~swmeanf$dat, pch=20)

```

#### Python

```{python p-sw-mean-flag}

swmeanf = con.execute('''
                     SELECT 
                        EXTRACT(DAYOFYEAR FROM endDateTime) AS dat,
                        SUM(flagabs) AS flagdaily
                     FROM (SELECT 
                              endDateTime,
                              ABS(VSWCFinalQF) AS flagabs
                           FROM swds)
                     GROUP BY 
                        EXTRACT(DAYOFYEAR FROM endDateTime)
                     ORDER BY dat
                     ''').df()

fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

ax1.plot(swmean.dat, swmean.swdaily)
ax2.scatter(swmeanf.dat, swmeanf.flagdaily, c='r')
plt.show()

```

### {-}

That is a lot of flags! To understand how to interpret these flags, and which daily mean values are reliable, we would need to examine the detailed quality flags. If we need a continuous time series, we would need to do some gap-filling of the omitted data. But our daily mean data don't include the full set of flags and values from the original data. Which brings us to the limitations and challenges of these methods.


# 2. Pitfalls, risks, and limitations

### Contextual data

Pulling data in this way makes it easier to overlook some of the details. For example, in the first query we ran on the mammal captures, we used `distinct()` to remove recaptures of the same individual. But in some cases, animals are recaptured and given a different identification, and we never accounted for that - those instances remain as duplicates in the data above. And we only know this because we included `tagID` in the columns we selected. Someone could easily leave out `tagID`, run `distinct()`, and think they are getting a clean estimate of species composition.

Similarly, there are repeat `tagID`s in the set of mammals with positive pathogen tests. They are a mix of repeat blood draws from the same individual, and individuals that tested positive for multiple pathogens.

(some kind of figure here?)

How to handle the re-identified recaptures and the pathogen repeats depends on your analysis, but both scenarios require something beyond simple data slicing and joining.

Counterpoint: It is also entirely possible to overlook the same details when analyzing downloaded data.

Counter-counterpoint: At least if you download everything, you can't entirely omit a critical column. If you go looking for contextual data, you already have it in memory.

### Aggregated data

Aggregating time series data vastly reduces the demand on compute and the time needed to access data. But it also introduces huge risks by discarding or averaging away missing or erroneous data points, usually invisibly.

The impact of ignoring gaps depends on your analysis, of course. But for any analysis that requires high precision and accuracy, careful attention to gap-filling is necessary.


# 3. Speed and performance


# 4. Recommendations


